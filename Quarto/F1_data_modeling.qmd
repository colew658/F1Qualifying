---
title: "F1 Project Data Modeling"
author: "Cole Wagner"
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    code-line-numbers: true
    code-tools: true
    cache: true
    theme: "yeti"
editor: visual
---

```{r setup}
library(tidyverse)
library(tidymodels)
library(janitor)
library(rstanarm)
library(multilevelmod)
library(loo)
library(zoo)
library(bayesplot)
library(rstan)
library(shinystan)
library(broom.mixed)
library(ranger)
library(finetune)
library(xgboost)
library(kableExtra)
library(iml)
set.seed(1)

full_no_missing <- read_csv("../Data/full_no_missing.csv")

full_clean_types <- full_no_missing %>%
  # Automatic conversion
  type_convert() %>%
  # Convert some numeric variables to factors
  mutate(
    MEETING_KEY = as.factor(MEETING_KEY),
    SESSION_KEY = as.factor(SESSION_KEY),
    DRIVER_NUMBER = as.factor(DRIVER_NUMBER),
    RAINFALL = as.factor(RAINFALL)
  )
```

# Apply the 107% rule to filter out "non-hot" laps

```{r filter107}
# Create a column to show if there was rain at any point in the session
# Also, find the cutoff time for each session
rain_cutoff <- full_clean_types %>%
  # Filter out any sessions that had rain
  group_by(SESSION_KEY) %>%
  summarise(
    rain_laps = sum(RAINFALL == 1),
    cutoff = min(LAP_DURATION) * 1.07
  ) %>%
  ungroup()

full_eng <- left_join(full_clean_types, rain_cutoff) %>%
  filter(LAP_DURATION < cutoff)
```

# Get Final Modeling Data

Select only the variables that will be used in the model.

```{r modeldat}
modeling_data <- full_eng %>%
  # Keep only the variables needed for modeling
  select(c(8, 13:17, 19, 23, 33:35))
```

# Base Model Pipeline

```{r basemodel}
base_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = F)

base_juice <- juice(prep(base_rec))

# Choose repeated 10-fold CV
kfold_rep <- vfold_cv(modeling_data, v = 10, repeats = 2, strata = "LAP_DURATION")

# Create linear regression model framework
f1_lm <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Build the modeling workflow
lm_workflow <- workflow() %>%
  add_model(f1_lm) %>%
  add_recipe(base_rec)

# Resampled error rate
lm_res <- fit_resamples(
  lm_workflow,
  resamples = kfold_rep,
  metrics = metric_set(rmse, mae, rsq)
)

lm_res %>% collect_metrics()

# Extract coefficients
lm_coefs <- lm_workflow %>%
  fit(modeling_data) %>%
  extract_fit_parsnip() %>%
  tidy()
```

# Check assumptions

```{r lmassumptions}
lm_native <- extract_fit_engine(fit(lm_workflow, modeling_data))

plot(lm_native)

# Normality is not great, but a log transformation makes it worse.
hist(modeling_data$LAP_DURATION)
hist(log(modeling_data$LAP_DURATION))

i <- 1
for (x in modeling_data[, -8]) {
  plot(
    x = x, y = modeling_data$LAP_DURATION,
    xlab = colnames(modeling_data[, -8])[i]
  )
  i <- i + 1
}
```

After looking at these plots, it appears that OLS linear regression is not the most appropriate model type for this data. Most of the predictors do not appear to be linearly related to the outcome, and the assumptions, especially the normality assumption, is shaky. As a result, I want to try two other frameworks: A Bayesian linear regression with random intercepts, and tree-based methods (random forest and xgboost). I do not want to throw out the results from the OLS regression, though, because the model is overall very good and has consistent performance across folds. As a result, I don't see these results as invalid.

# Bayesian Linear Regression with Random Intercepts
Based on the fact that each track is so unique, my background knowledge leads me to believe that a random intercept for each track could improve model performance.

```{r bayes}
randint_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  # This adds random intercept for circuit
  add_role(CIRCUIT_SHORT_NAME, new_role = "exp_unit") %>%
  step_normalize(all_numeric_predictors(), -has_role("exp_unit")) %>%
  step_dummy(all_nominal_predictors(), -has_role("exp_unit"))

# Set up the Bayesian engine and priors
glmer_mod <- linear_reg() %>%
  set_engine("stan_glmer",
    cores = 4
  )

randint_wf <- workflow() %>%
  add_recipe(randint_rec) %>%
  # Need to specify formula for the model
  add_model(glmer_mod, formula = LAP_DURATION ~ AIR_TEMPERATURE + HUMIDITY +
    PRESSURE + RAINFALL_X1 + TRACK_TEMPERATURE + WIND_SPEED + TURNS +
    LENGTH_KM + rain_laps + (1 | CIRCUIT_SHORT_NAME))

# Resampled error rate
# randint_res <- fit_resamples(
#   randint_wf,
#   resamples = kfold_rep,
#   metrics = metric_set(rmse, mae, rsq)
# )
# saveRDS(randint_res, "../Data/randint_res.rds")
randint_res <- readRDS("../Data/randint_res.rds")
# Look at performance metrics
randint_res %>% collect_metrics()

# Extract native model fit
# randint_extract <- extract_fit_engine(randint_wf %>% fit(modeling_data))
# saveRDS(randint_extract, "../Data/randint_extract.rds")
randint_extract <- readRDS("../Data/randint_extract.rds")

# launch_shinystan(randint_extract)
```

After examining diagnostics via shinystan, the assumptions seem to be met.

# Tree-Based Methods

## Random Forest
```{r ranger}
# Create a random forest model
rf_mod <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger", num.threads = 7)

rf_wf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_mod)

# Create a grid of tuning parameters
rf_param <- extract_parameter_set_dials(rf_mod) %>%
  update(mtry = mtry(c(1, 32)))

rf_tunegrid <- grid_space_filling(rf_param, size = 5)

# Tune the model
# rf_tune <- tune_race_anova(
#   rf_wf,
#   resamples = kfold_rep,
#   grid = rf_tunegrid,
#   metrics = metric_set(rmse, mae, rsq)
# )
#
# saveRDS(rf_tune, "../Data/rf_tune.rds")
rf_tune <- readRDS("../Data/rf_tune.rds")

rf_besttune <- select_best(rf_tune, metric = "rmse")

rf_finalmod <- finalize_model(rf_mod, rf_besttune)

rf_finalwf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_finalmod)

rbind(rf_tune %>% show_best(metric = "rmse"),
rf_tune %>% show_best(metric = "mae"),
rf_tune %>% show_best(metric = "rsq"))
```

## XGBoost
```{r xgb}
xgb_mod <- boost_tree(
  mode = "regression",
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 7)

xgb_param <- extract_parameter_set_dials(xgb_mod) %>%
  update(mtry = mtry(c(1, 32)))

xgb_tunegrid <- grid_space_filling(xgb_param, size = 5)

xgb_wf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_mod)


# xgb_tune <- tune_race_anova(
#   xgb_wf,
#   resamples = kfold_rep,
#   grid = xgb_tunegrid,
#   metrics = metric_set(rmse, mae, rsq)
# )
# 
# saveRDS(xgb_tune, "../Data/xgb_tune.rds")
xgb_tune <- readRDS("../Data/xgb_tune.rds")

xgb_besttune <- select_best(xgb_tune, metric = "rmse")

xgb_finalmod <- finalize_model(xgb_mod, xgb_besttune)

xgb_finalwf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_finalmod)

rbind(xgb_tune %>% show_best(metric = "rmse"),
xgb_tune %>% show_best(metric = "mae"),
xgb_tune %>% show_best(metric = "rsq"))
```

# Model Evaluation
```{r modeleval}
# format all metrics tables
lm_metrics <- lm_res %>%
  collect_metrics() %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Base Linear Regression (mean)", "Base Linear Regression (std)"), .before = name) %>%
  select(-name)

randint_metrics <- randint_res %>%
  collect_metrics() %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Bayesian LR w/ Random Intercept (mean)", "Bayesian LR w/ Random Intercept (std)"), .before = name) %>%
  select(-name)

rf_metrics <- rbind(rf_tune %>% show_best(metric = "rmse"),
rf_tune %>% show_best(metric = "mae"),
rf_tune %>% show_best(metric = "rsq")) %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Random Forest (mean)", "Random Forest (std)"), .before = name) %>%
  select(-name)

xgb_metrics <- rbind(xgb_tune %>% show_best(metric = "rmse"),
xgb_tune %>% show_best(metric = "mae"),
xgb_tune %>% show_best(metric = "rsq")) %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("XGBoost (mean)", "XGBoost (std)"), .before = name) %>%
  select(-name)

# combine all metrics tables and create a kable object
comb_metrics <- rbind(lm_metrics, randint_metrics, rf_metrics, xgb_metrics) %>%
  rename(RMSE = rmse, MAE = mae, Rsquared = rsq)

metrics_table <- kable(comb_metrics, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
metrics_table

```

Because the random forest model is best, I will further examine this model.

# Random Forest Examination

## Variable Importance Plot
```{r impplot}
# Native random forest model
rf_extract <- extract_fit_engine(rf_finalwf %>% fit(modeling_data))

# Create prediction function
pfun_ranger <- function(object, newdata) predict(object, data = newdata)$predictions

# Create a base iml object that is needed before generating additional plots
predictor_ranger <- Predictor$new(model = rf_extract, 
                                  data = base_juice[,-c(9)], 
                                  y = base_juice[,9], predict.fun = pfun_ranger)

# imp_rf <- FeatureImp$new(predictor_ranger, loss = "rmse")
# saveRDS(imp_rf, "../Data/imp_rf.rds")
imp_rf <- readRDS("../Data/imp_rf.rds")
plot(imp_rf)
```

## ALE Plots
```{r ale}
rf_ale_length <- FeatureEffect$new(predictor_ranger, feature = "LENGTH_KM", method='ale')
rf_ale_length$plot()

rf_ale_turns <- FeatureEffect$new(predictor_ranger, feature = "TURNS", method='ale')
rf_ale_turns$plot()

rf_ale_humidity <- FeatureEffect$new(predictor_ranger, feature = "HUMIDITY", method='ale')
rf_ale_humidity$plot()

rf_ale_pressure <- FeatureEffect$new(predictor_ranger, feature = "PRESSURE", method='ale')
rf_ale_pressure$plot()
```








