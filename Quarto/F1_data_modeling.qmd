---
title: "F1 Project Data Modeling"
author: "Cole Wagner"
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    code-line-numbers: true
    code-tools: true
    cache: true
    theme: "yeti"
editor: visual
---

```{r setup}
library(tidyverse)
library(tidymodels)
library(janitor)
library(rstanarm)
library(multilevelmod)
library(loo)
library(zoo)
library(bayesplot)
library(rstan)
library(shinystan)
library(broom.mixed)
set.seed(1)

full_no_missing <- read_csv("../Data/full_no_missing.csv")

full_clean_types <- full_no_missing %>%
  # Automatic conversion
  type_convert() %>%
  # Convert some numeric variables to factors
  mutate(
    MEETING_KEY = as.factor(MEETING_KEY),
    SESSION_KEY = as.factor(SESSION_KEY),
    DRIVER_NUMBER = as.factor(DRIVER_NUMBER),
    RAINFALL = as.factor(RAINFALL)
  )
```

# Apply the 107% rule to filter out "non-hot" laps

```{r filter107}
# Create a column to show if there was rain at any point in the session
# Also, find the cutoff time for each session
rain_cutoff <- full_clean_types %>%
  # Filter out any sessions that had rain
  group_by(SESSION_KEY) %>%
  summarise(
    rain_laps = sum(RAINFALL == 1),
    cutoff = min(LAP_DURATION) * 1.07
  ) %>%
  ungroup()

full_eng <- left_join(full_clean_types, rain_cutoff) %>%
  filter(LAP_DURATION < cutoff)
```

# Get Final Modeling Data

Select only the variables that will be used in the model.

```{r modeldat}
modeling_data <- full_eng %>%
  # Keep only the variables needed for modeling
  select(c(8, 13:17, 19, 23, 33:35))
```

# Base Model Pipeline

```{r basemodel}
base_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = F)

# Choose repeated 10-fold CV
kfold_rep <- vfold_cv(modeling_data, v = 10, repeats = 2, strata = "LAP_DURATION")

# Create linear regression model framework
f1_lm <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Build the modeling workflow
lm_workflow <- workflow() %>%
  add_model(f1_lm) %>%
  add_recipe(base_rec)

# Resampled error rate
lm_res <- fit_resamples(
  lm_workflow,
  resamples = kfold_rep,
  metrics = metric_set(rmse, mae, rsq)
)

lm_res %>% collect_metrics()

# Extract coefficients
lm_coefs <- lm_workflow %>%
  fit(modeling_data) %>%
  extract_fit_parsnip() %>%
  tidy()
```

# Check assumptions

```{r lmassumptions}
lm_native <- extract_fit_engine(fit(lm_workflow, modeling_data))

plot(lm_native)

# Normality is not great, but a log transformation makes it worse.
hist(modeling_data$LAP_DURATION)
hist(log(modeling_data$LAP_DURATION))

i <- 1
for (x in modeling_data[, -8]) {
  plot(
    x = x, y = modeling_data$LAP_DURATION,
    xlab = colnames(modeling_data[, -8])[i]
  )
  i <- i + 1
}
```

After looking at these plots, it appears that OLS linear regression is not the most appropriate model type for this data. Most of the predictors do not appear to be linearly related to the outcome, and the assumptions, especially the normality assumption, is shaky. As a result, I want to try two other frameworks: A Bayesian linear regression with random intercepts, and tree-based methods (random forest and xgboost). I do not want to throw out the results from the OLS regression, though, because the model is overall very good and has consistent performance across folds. As a result, I don't see these results as invalid.

# Bayesian Linear Regression with Random Intercepts
Based on the fact that each track is so unique, my background knowledge leads me to believe that a random intercept for each track could improve model performance.

```{r bayes}
randint_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  # This adds random intercept for circuit
  add_role(CIRCUIT_SHORT_NAME, new_role = "exp_unit") %>%
  step_normalize(all_numeric_predictors(), -has_role("exp_unit")) %>%
  step_dummy(all_nominal_predictors(), -has_role("exp_unit"))

# Set up the Bayesian engine and priors
glmer_mod <- linear_reg() %>%
  set_engine("stan_glmer",
    cores = 4
  )

randint_wf <- workflow() %>%
  add_recipe(randint_rec) %>%
  # Need to specify formula for the model
  add_model(glmer_mod, formula = LAP_DURATION ~ AIR_TEMPERATURE + HUMIDITY +
              PRESSURE + RAINFALL_X1 + TRACK_TEMPERATURE + WIND_SPEED + TURNS +
              LENGTH_KM + rain_laps + (1 | CIRCUIT_SHORT_NAME))

# Resampled error rate
# randint_res <- fit_resamples(
#   randint_wf,
#   resamples = kfold_rep,
#   metrics = metric_set(rmse, mae, rsq)
# )
# saveRDS(randint_res, "../Data/randint_res.rds")
randint_res <- readRDS("../Data/randint_res.rds")
# Look at performance metrics
randint_res %>% collect_metrics()

# Extract native model fit
# randint_extract <- extract_fit_engine(randint_wf %>% fit(modeling_data))
# saveRDS(randint_extract, "../Data/randint_extract.rds")
randint_extract <- readRDS("../Data/randint_extract.rds")

launch_shinystan(randint_extract)
```






