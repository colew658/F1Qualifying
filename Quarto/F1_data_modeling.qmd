---
title: "F1 Project Data Modeling"
author: "Cole Wagner"
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    code-line-numbers: true
    code-tools: true
    cache: true
    theme: "yeti"
editor: visual
---

```{r setup}
library(tidyverse)
library(tidymodels)
library(janitor)
library(rstanarm)
library(multilevelmod)
library(loo)
library(zoo)
library(bayesplot)
library(rstan)
library(shinystan)
library(broom.mixed)
library(ranger)
library(finetune)
library(xgboost)
library(kableExtra)
library(iml)
library(reactable)
library(reactablefmtr)
library(htmltools)
set.seed(1)

full_no_missing <- read_csv("../Data/full_no_missing.csv")

full_clean_types <- full_no_missing %>%
  # Automatic conversion
  type_convert() %>%
  # Convert some numeric variables to factors
  mutate(
    MEETING_KEY = as.factor(MEETING_KEY),
    SESSION_KEY = as.factor(SESSION_KEY),
    DRIVER_NUMBER = as.factor(DRIVER_NUMBER),
    RAINFALL = as.factor(RAINFALL)
  )
```

# Apply the 107% rule to filter out "non-hot" laps

```{r filter107}
# Create a column to show if there was rain at any point in the session
# Also, find the cutoff time for each session
rain_cutoff <- full_clean_types %>%
  # Filter out any sessions that had rain
  group_by(SESSION_KEY) %>%
  summarise(
    rain_laps = sum(RAINFALL == 1),
    cutoff = min(LAP_DURATION) * 1.07
  ) %>%
  ungroup()

full_eng <- left_join(full_clean_types, rain_cutoff) %>%
  filter(LAP_DURATION < cutoff)
```

# Get Final Modeling Data

Select only the variables that will be used in the model.

```{r modeldat}
modeling_data <- full_eng %>%
  # Keep only the variables needed for modeling
  select(c(8, 13:17, 19, 23, 33:35))
```

# Base Model Pipeline

```{r basemodel}
base_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = F)

base_juice <- juice(prep(base_rec))

# Choose repeated 10-fold CV
kfold_rep <- vfold_cv(modeling_data, v = 10, repeats = 2, strata = "LAP_DURATION")

# Create linear regression model framework
f1_lm <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Build the modeling workflow
lm_workflow <- workflow() %>%
  add_model(f1_lm) %>%
  add_recipe(base_rec)

# Resampled error rate
lm_res <- fit_resamples(
  lm_workflow,
  resamples = kfold_rep,
  metrics = metric_set(rmse, mae, rsq)
)

lm_res %>% collect_metrics()

# Extract coefficients
lm_coefs <- lm_workflow %>%
  fit(modeling_data) %>%
  extract_fit_parsnip() %>%
  tidy()
```

# Check assumptions

```{r lmassumptions}
lm_native <- extract_fit_engine(fit(lm_workflow, modeling_data))

plot(lm_native)

# Normality is not great, but a log transformation makes it worse.
hist(modeling_data$LAP_DURATION)
hist(log(modeling_data$LAP_DURATION))

i <- 1
for (x in modeling_data[, -8]) {
  plot(
    x = x, y = modeling_data$LAP_DURATION,
    xlab = colnames(modeling_data[, -8])[i]
  )
  i <- i + 1
}
```

After looking at these plots, it appears that OLS linear regression is not the most appropriate model type for this data. Most of the predictors do not appear to be linearly related to the outcome, and the assumptions, especially the normality assumption, is shaky. As a result, I want to try two other frameworks: A Bayesian linear regression with random intercepts, and tree-based methods (random forest and xgboost). I do not want to throw out the results from the OLS regression, though, because the model is overall very good and has consistent performance across folds. As a result, I don't see these results as invalid.

# Bayesian Linear Regression with Random Intercepts

Based on the fact that each track is so unique, my background knowledge leads me to believe that a random intercept for each track could improve model performance.

```{r bayes}
randint_rec <- recipe(LAP_DURATION ~ ., data = modeling_data) %>%
  # This adds random intercept for circuit
  add_role(CIRCUIT_SHORT_NAME, new_role = "exp_unit") %>%
  step_normalize(all_numeric_predictors(), -has_role("exp_unit")) %>%
  step_dummy(all_nominal_predictors(), -has_role("exp_unit"))

# Set up the Bayesian engine and priors
glmer_mod <- linear_reg() %>%
  set_engine("stan_glmer",
    cores = 4
  )

randint_wf <- workflow() %>%
  add_recipe(randint_rec) %>%
  # Need to specify formula for the model
  add_model(glmer_mod, formula = LAP_DURATION ~ AIR_TEMPERATURE + HUMIDITY +
    PRESSURE + RAINFALL_X1 + TRACK_TEMPERATURE + WIND_SPEED + TURNS +
    LENGTH_KM + rain_laps + (1 | CIRCUIT_SHORT_NAME))

# Resampled error rate
# randint_res <- fit_resamples(
#   randint_wf,
#   resamples = kfold_rep,
#   metrics = metric_set(rmse, mae, rsq)
# )
# saveRDS(randint_res, "../Data/randint_res.rds")
randint_res <- readRDS("../Data/randint_res.rds")
# Look at performance metrics
randint_res %>% collect_metrics()

# Extract native model fit
# randint_extract <- extract_fit_engine(randint_wf %>% fit(modeling_data))
# saveRDS(randint_extract, "../Data/randint_extract.rds")
randint_extract <- readRDS("../Data/randint_extract.rds")

# launch_shinystan(randint_extract)
```

After examining diagnostics via shinystan, the assumptions seem to be met.

# Tree-Based Methods

## Random Forest

```{r ranger}
# Create a random forest model
rf_mod <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger", num.threads = 7)

rf_wf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_mod)

# Create a grid of tuning parameters
rf_param <- extract_parameter_set_dials(rf_mod) %>%
  update(mtry = mtry(c(1, 32)))

rf_tunegrid <- grid_space_filling(rf_param, size = 5)

# Tune the model
# rf_tune <- tune_race_anova(
#   rf_wf,
#   resamples = kfold_rep,
#   grid = rf_tunegrid,
#   metrics = metric_set(rmse, mae, rsq)
# )
#
# saveRDS(rf_tune, "../Data/rf_tune.rds")
rf_tune <- readRDS("../Data/rf_tune.rds")

rf_besttune <- select_best(rf_tune, metric = "rmse")

rf_finalmod <- finalize_model(rf_mod, rf_besttune)

rf_finalwf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_finalmod)

rbind(rf_tune %>% show_best(metric = "rmse"),
rf_tune %>% show_best(metric = "mae"),
rf_tune %>% show_best(metric = "rsq"))
```

## XGBoost

```{r xgb}
xgb_mod <- boost_tree(
  mode = "regression",
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 7)

xgb_param <- extract_parameter_set_dials(xgb_mod) %>%
  update(mtry = mtry(c(1, 32)))

xgb_tunegrid <- grid_space_filling(xgb_param, size = 5)

xgb_wf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_mod)


# xgb_tune <- tune_race_anova(
#   xgb_wf,
#   resamples = kfold_rep,
#   grid = xgb_tunegrid,
#   metrics = metric_set(rmse, mae, rsq)
# )
# 
# saveRDS(xgb_tune, "../Data/xgb_tune.rds")
xgb_tune <- readRDS("../Data/xgb_tune.rds")

xgb_besttune <- select_best(xgb_tune, metric = "rmse")

xgb_finalmod <- finalize_model(xgb_mod, xgb_besttune)

xgb_finalwf <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_finalmod)

rbind(xgb_tune %>% show_best(metric = "rmse"),
xgb_tune %>% show_best(metric = "mae"),
xgb_tune %>% show_best(metric = "rsq"))
```

# Model Evaluation

```{r modeleval}
# format all metrics tables
lm_metrics <- lm_res %>%
  collect_metrics() %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Base Linear Regression (mean)", "Base Linear Regression (std)"), .before = name) %>%
  select(-name)

randint_metrics <- randint_res %>%
  collect_metrics() %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Bayesian LR w/ Random Intercept (mean)", "Bayesian LR w/ Random Intercept (std)"), .before = name) %>%
  select(-name)

rf_metrics <- rbind(rf_tune %>% show_best(metric = "rmse"),
rf_tune %>% show_best(metric = "mae"),
rf_tune %>% show_best(metric = "rsq")) %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("Random Forest (mean)", "Random Forest (std)"), .before = name) %>%
  select(-name)

xgb_metrics <- rbind(xgb_tune %>% show_best(metric = "rmse"),
xgb_tune %>% show_best(metric = "mae"),
xgb_tune %>% show_best(metric = "rsq")) %>%
  select(.metric, mean, std_err) %>%
  pivot_longer(-1) %>%
  pivot_wider(names_from = 1, values_from = value) %>%
  mutate(Model = c("XGBoost (mean)", "XGBoost (std)"), .before = name) %>%
  select(-name)

# combine all metrics tables and create a kable object
comb_metrics <- rbind(lm_metrics, randint_metrics, rf_metrics, xgb_metrics) %>%
  rename(RMSE = rmse, MAE = mae, Rsquared = rsq)

metrics_table <- kable(comb_metrics, digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
metrics_table

```

Because the random forest model is best, I will further examine this model.

# Random Forest Examination

## Variable Importance Plot

```{r impplot}
# Native random forest model
rf_extract <- extract_fit_engine(rf_finalwf %>% fit(modeling_data))

# Create prediction function
pfun_ranger <- function(object, newdata) predict(object, data = newdata)$predictions

# Create a base iml object that is needed before generating additional plots
predictor_ranger <- Predictor$new(model = rf_extract, 
                                  data = base_juice[,-c(9)], 
                                  y = base_juice[,9], predict.fun = pfun_ranger)

# imp_rf <- FeatureImp$new(predictor_ranger, loss = "rmse")
# saveRDS(imp_rf, "../Data/imp_rf.rds")
imp_rf <- readRDS("../Data/imp_rf.rds")
plot(imp_rf)
```

## ALE Plots

```{r ale}
rf_ale_length <- FeatureEffect$new(predictor_ranger, feature = "LENGTH_KM", method='ale')
rf_ale_length$plot()

rf_ale_turns <- FeatureEffect$new(predictor_ranger, feature = "TURNS", method='ale')
rf_ale_turns$plot()

rf_ale_humidity <- FeatureEffect$new(predictor_ranger, feature = "HUMIDITY", method='ale')
rf_ale_humidity$plot()

rf_ale_pressure <- FeatureEffect$new(predictor_ranger, feature = "PRESSURE", method='ale')
rf_ale_pressure$plot()

rf_ale_airtemp <- FeatureEffect$new(predictor_ranger, feature = "AIR_TEMPERATURE", method='ale')
rf_ale_airtemp$plot()

rf_ale_tracktemp <- FeatureEffect$new(predictor_ranger, feature = "TRACK_TEMPERATURE", method='ale')
rf_ale_tracktemp$plot()
```

## Run Model With Only Weather Predictors

```{r weatheronly}
# Remove track attributes from the data
weatheronly_data <- modeling_data[, -c(8:10)]

weatheronly_rec <- recipe(LAP_DURATION ~ ., data = weatheronly_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = F)

weatheronly_juice <- juice(prep(weatheronly_rec))
  
# Create a random forest model
weatheronly_mod <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger", num.threads = 7)

weatheronly_wf <- workflow() %>%
  add_recipe(weatheronly_rec) %>%
  add_model(weatheronly_mod)

# Create a grid of tuning parameters
weatheronly_param <- extract_parameter_set_dials(weatheronly_mod) %>%
  update(mtry = mtry(c(1, 7)))

weatheronly_tunegrid <- grid_space_filling(weatheronly_param, size = 5)

# Create a new CV
weatheronly_rep <- vfold_cv(weatheronly_data, v = 10, repeats = 2, strata = "LAP_DURATION")

# Tune the model
# weatheronly_tune <- tune_race_anova(
#   weatheronly_wf,
#   resamples = weatheronly_rep,
#   grid = weatheronly_tunegrid,
#   metrics = metric_set(yardstick::rmse, yardstick::mae, yardstick::rsq)
# )

# saveRDS(weatheronly_tune, "../Data/weatheronly_tune.rds")
weatheronly_tune <- readRDS("../Data/weatheronly_tune.rds")

weatheronly_besttune <- select_best(weatheronly_tune, metric = "rmse")

weatheronly_finalmod <- finalize_model(weatheronly_mod, weatheronly_besttune)

weatheronly_finalwf <- workflow() %>%
  add_recipe(weatheronly_rec) %>%
  add_model(weatheronly_finalmod)

rbind(weatheronly_tune %>% show_best(metric = "rmse"),
weatheronly_tune %>% show_best(metric = "mae"),
weatheronly_tune %>% show_best(metric = "rsq"))
```

### Variable Importance
```{r weatherimp}
# Native random forest model
weatheronly_extract <- extract_fit_engine(weatheronly_finalwf %>% fit(weatheronly_data))

# Create prediction function
pfun_ranger <- function(object, newdata) predict(object, data = newdata)$predictions

# Create a base iml object that is needed before generating additional plots
predictor_ranger <- Predictor$new(model = weatheronly_extract, 
                                  data = weatheronly_juice[, -7], 
                                  y = weatheronly_juice[, 7], predict.fun = pfun_ranger)

# imp_weatheronly <- FeatureImp$new(predictor_ranger, loss = "rmse")
# saveRDS(imp_weatheronly, "../Data/imp_weatheronly.rds")
imp_weatheronly <- readRDS("../Data/imp_weatheronly.rds")
plot(imp_weatheronly)
```

### ALE Plots
```{r weatherale}
weatheronly_ale_humidity <- FeatureEffect$new(predictor_ranger, feature = "HUMIDITY", method='ale')
weatheronly_ale_humidity$plot()
# saveRDS(weatheronly_ale_humidity, "../Data/humidity_ale.rds")

weatheronly_ale_pressure <- FeatureEffect$new(predictor_ranger, feature = "PRESSURE", method='ale')
weatheronly_ale_pressure$plot()
# saveRDS(weatheronly_ale_pressure, "../Data/pressure_ale.rds")

weatheronly_ale_airtemp <- FeatureEffect$new(predictor_ranger, feature = "AIR_TEMPERATURE", method='ale')
weatheronly_ale_airtemp$plot()
# saveRDS(weatheronly_ale_airtemp, "../Data/airtemp_ale.rds")

weatheronly_ale_tracktemp <- FeatureEffect$new(predictor_ranger, feature = "TRACK_TEMPERATURE", method='ale')
weatheronly_ale_tracktemp$plot()
# saveRDS(weatheronly_ale_tracktemp, "../Data/tracktemp_ale.rds")

weatheronly_ale_windspeed <- FeatureEffect$new(predictor_ranger, feature = "WIND_SPEED", method='ale')
weatheronly_ale_windspeed$plot()
# saveRDS(weatheronly_ale_windspeed, "../Data/windspeed_ale.rds")

weatheronly_ale_rainlaps <- FeatureEffect$new(predictor_ranger, feature = "rain_laps", method='ale')
weatheronly_ale_rainlaps$plot()
# saveRDS(weatheronly_ale_rainlaps, "../Data/rainlaps_ale.rds")

weatheronly_ale_rainfall <- FeatureEffect$new(predictor_ranger, feature = "RAINFALL_X1", method='ale')
weatheronly_ale_rainfall$plot()
# saveRDS(weatheronly_ale_rainfall, "../Data/rainfall_ale.rds")
```

## Remove Mexico City Laps Because of Extreme Low Pressure
```{r nomex}
# Remove Mexico City laps from data
nomex_data <- modeling_data[modeling_data$CIRCUIT_SHORT_NAME != "Mexico City", -c(8:10)]

nomex_rec <- recipe(LAP_DURATION ~ ., data = nomex_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = F)

nomex_juice <- juice(prep(nomex_rec))
  
# Create a random forest model
nomex_mod <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger", num.threads = 7)

nomex_wf <- workflow() %>%
  add_recipe(nomex_rec) %>%
  add_model(nomex_mod)

# Create a grid of tuning parameters
nomex_param <- extract_parameter_set_dials(nomex_mod) %>%
  update(mtry = mtry(c(1, 7)))

nomex_tunegrid <- grid_space_filling(nomex_param, size = 5)

# Create a new CV
nomex_rep <- vfold_cv(nomex_data, v = 10, repeats = 2, strata = "LAP_DURATION")

# Tune the model
# nomex_tune <- tune_race_anova(
#   nomex_wf,
#   resamples = nomex_rep,
#   grid = nomex_tunegrid,
#   metrics = metric_set(yardstick::rmse, yardstick::mae, yardstick::rsq)
# )

# saveRDS(nomex_tune, "../Data/nomex_tune.rds")
nomex_tune <- readRDS("../Data/nomex_tune.rds")

nomex_besttune <- select_best(nomex_tune, metric = "rmse")

nomex_finalmod <- finalize_model(nomex_mod, nomex_besttune)

nomex_finalwf <- workflow() %>%
  add_recipe(nomex_rec) %>%
  add_model(nomex_finalmod)

rbind(nomex_tune %>% show_best(metric = "rmse"),
nomex_tune %>% show_best(metric = "mae"),
nomex_tune %>% show_best(metric = "rsq"))

```
### Variable Importance
```{r nomeximp}
# Native random forest model
nomex_extract <- extract_fit_engine(nomex_finalwf %>% fit(nomex_data))

# Create prediction function
pfun_ranger <- function(object, newdata) predict(object, data = newdata)$predictions

# Create a base iml object that is needed before generating additional plots
predictor_ranger <- Predictor$new(model = nomex_extract, 
                                  data = nomex_juice[, -7], 
                                  y = nomex_juice[, 7], predict.fun = pfun_ranger)

# imp_nomex <- FeatureImp$new(predictor_ranger, loss = "rmse")
# saveRDS(imp_nomex, "../Data/imp_nomex.rds")
imp_nomex <- readRDS("../Data/imp_nomex.rds")
plot(imp_nomex)
```
### ALE Plot for Pressure
```{r pressureale}
nomex_ale_pressure <- FeatureEffect$new(predictor_ranger, feature = "PRESSURE", method='ale')
nomex_ale_pressure$plot()
```


# Summary Tables

## Generate Predictions to Create Summary Dataset
```{r preds}
# Fit the final model on the modeling data
final_fit <- fit(weatheronly_finalwf, weatheronly_data)
# saveRDS(final_fit, "../Data/final_fit.rds")

# Generate predictions on the full data
preds <- predict(final_fit, full_eng)
# Add predictions to full dataset
full_preds <- cbind(full_eng, preds) %>%
  # Calculate the difference of actual and predicted
  mutate(time_diff = LAP_DURATION - .pred) %>%
  # Standardize column names
  clean_names(case = "all_caps")
```

## Create Driver Summary Table
```{r driversummary}
driver_summary <- full_preds %>%
  group_by(FULL_NAME) %>%
  summarise(
    # Total laps
    `Number of Laps` = n(),
    # Average time difference (actual - predicted)
    `Average Time Difference` = mean(TIME_DIFF),
    # Percentage of laps where the driver outperformed the prediction
    `Percent of Laps Outperformed Expectation` = sum(LAP_DURATION < PRED) / n()
    ) %>%
  ungroup() %>%
  # Sort by average time difference (best drivers on top)
  arrange(`Average Time Difference`) %>%
  rename(`Driver Name` = FULL_NAME)
```

### Reactable
```{r driverreact}
driver_reactable <- reactable(
  driver_summary,
  # Round all numeric variables to 2 digits
  defaultColDef = colDef(format = colFormat(digit = 2)),
  striped = T,
  highlight = T,
  bordered = T,
  defaultPageSize = 30,
  theme = reactableTheme(
    highlightColor = "#fa9696"
  )
  )

driver_reactable

# saveRDS(driver_reactable, "../Data/driver_reactable.rds")
```

## "Best" Laps Table
```{r bestlaps}
best_laps <- full_preds %>%
  # Sort by time difference
  arrange(TIME_DIFF) %>%
  # Take only the top 10 laps
  head(n = 10) %>%
  # Select only relevant variables
  select(8, 13:17, 19:20, 23, 29, 37:38) %>%
  # Move actual lap time next to predicted for ease of comparison
  relocate(LAP_DURATION, .after = YEAR) %>%
  rename(`Air Temp (°C)` = AIR_TEMPERATURE, `Relative Humidity (%)` = HUMIDITY, 
         `Pressure (mbar)` = PRESSURE, `Rain` = RAINFALL,
         `Track Temp (°C)` = TRACK_TEMPERATURE, `Wind Speed (m/s)` = WIND_SPEED,
         `Driver Name` = FULL_NAME, `Circuit` = CIRCUIT_SHORT_NAME,
         `Year` = YEAR, `Lap Time (s)` = LAP_DURATION,
         `Predicted Lap Time` = PRED, `Time Difference` = TIME_DIFF)
```

### Reactable
```{r lapreact}
lap_reactable <- reactable(
  best_laps,
  # Round all numeric variables to two digits
  defaultColDef = colDef(format = colFormat(digit = 2)),
  # Add custom formatting for some columns
  columns = list(
    `Driver Name` = colDef(width = 135),
    Year = colDef(format = colFormat(digits = 0))
  ),
  striped = T,
  highlight = T,
  bordered = T,
  theme = reactableTheme(
    borderColor = "#959595",
    stripedColor = "ivory3",
    highlightColor = "#98F5FF",
    cellPadding = "8px 12px",
    searchInputStyle = list(width = "100%")
  )
  )

lap_reactable

# saveRDS(lap_reactable, "../Data/lap_reactable.rds")
```












